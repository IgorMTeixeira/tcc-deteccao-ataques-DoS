{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7294b78-2db2-42b8-bc20-fba12b144363",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1  -  Geração de Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1664faf-d571-4dfa-8dff-994639d4412e",
   "metadata": {},
   "source": [
    "## 1.1 - Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28fa49-ef7e-4f7b-8172-35b600ea3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cb9a9-5381-4f85-9e11-14643a1105aa",
   "metadata": {},
   "source": [
    "## 1.2 - Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96fbe12-d455-4949-8548-3ba1b350995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar dados de um arquivo CSV\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149c673-fe2f-4446-b293-18e4aa824f6d",
   "metadata": {},
   "source": [
    "## 1.3 - Pré-processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee77a88-f12d-47d8-89af-ee7859f04ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coluna 'class' restaurada com ['normal' 'anomaly'] valores exclusivos.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_training_data(df, columns_to_remove=None, fit=True):\n",
    "    if columns_to_remove is None:\n",
    "        columns_to_remove = [\n",
    "            'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
    "            'su_attempted', 'num_root', 'num_file_creations', 'num_shells', \n",
    "            'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login'\n",
    "        ]\n",
    "\n",
    "    df.drop(columns=columns_to_remove, axis=1, errors='ignore', inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    if 'class' in df.columns:\n",
    "        class_column = df.pop('class')  # Remove 'class' temporariamente\n",
    "    else:\n",
    "        class_column = None\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Adiciona a coluna 'class' de volta ao DataFrame\n",
    "    if class_column is not None:\n",
    "        df['class'] = class_column.values\n",
    "\n",
    "    return df, encoder, imputer\n",
    "\n",
    "def preprocess_new_data(df, encoder, imputer, scaler, numeric_cols):\n",
    "    df.drop(['src_ip', 'dst_ip', 'timestamp', 'src_port', 'dst_port', 'frame_length'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "    if 'class' in df.columns:\n",
    "        class_column = df.pop('class')  # Remove 'class' temporariamente\n",
    "    else:\n",
    "        class_column = None\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Preencher valores ausentes usando o imputer ajustado\n",
    "    df[numeric_cols] = imputer.transform(df[numeric_cols])\n",
    "\n",
    "    # Codificação One-Hot com o encoder ajustado\n",
    "    encoded_categorical = pd.DataFrame(\n",
    "        encoder.transform(df[categorical_cols]),\n",
    "        columns=encoder.get_feature_names_out(categorical_cols),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    df.drop(columns=categorical_cols, inplace=True)\n",
    "    df = pd.concat([df, encoded_categorical], axis=1)\n",
    "\n",
    "    # Escalonamento com o scaler ajustado\n",
    "    df[numeric_cols] = scaler.transform(df[numeric_cols])\n",
    "\n",
    "    # Adiciona a coluna 'class' de volta ao DataFrame\n",
    "    if class_column is not None:\n",
    "        df['class'] = class_column.values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584be0f-cc35-4cce-be06-54e0365c4eb5",
   "metadata": {},
   "source": [
    "## 1.4 - Criação do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ab032-cd9a-4ee6-aad2-35e674a2f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(X_train):\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5c716-da14-4e5f-8a07-6d3a7fe124ac",
   "metadata": {},
   "source": [
    "## 1.5 - Divisão e Balanceamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91a6d1b-fa72-443a-aa42-2a1b66dc3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para dividir e balancear os dados usando SMOTE\n",
    "def split_and_balance_data(df, method='smote'):\n",
    "    X = df.drop(['class'], axis=1)\n",
    "    y = df['class']\n",
    "    if method == 'smote':\n",
    "        oversampler = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "    return train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc7f40-9322-47dd-aaba-f3f317d17261",
   "metadata": {},
   "source": [
    "## 1.6 - Alinhamento de Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15bd461-d519-4fd4-9ca0-1f38efd49ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_add_missing_columns(df, reference_columns):\n",
    "    missing_cols = set(reference_columns) - set(df.columns)\n",
    "    for col in missing_cols:\n",
    "        df[col] = 0\n",
    "    df = df[reference_columns]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6682d43-6466-4bef-a3fb-7cd7740ea747",
   "metadata": {},
   "source": [
    "## 1.7 - Otimização de Hiperparâmetros do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67869a80-d0f1-4810-ba07-cda958dd4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para otimizar um modelo usando pipeline e busca em grade\n",
    "def optimize_model_with_pipeline(X_train, y_train):\n",
    "    pipeline = create_pipeline(X_train)\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 150],\n",
    "        'classifier__max_features': ['sqrt', 'log2'],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "     # Configura a busca em grade com validação cruzada (cv=5) e acurácia como métrica\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Melhores hiperparâmetros: {grid_search.best_params_}')\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf493c1-94bf-454c-bba8-25ae2db6f49d",
   "metadata": {},
   "source": [
    "## 1.8 - Treinamento com Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298ca95-1987-4df0-8dc8-64bab32f5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treina o modelo com validação cruzada\n",
    "def train_model_with_cross_validation(X_train, y_train):\n",
    "    rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "     \n",
    "    # Utiliza StratifiedKFold para cross-validation para dividir os dados em partes\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "    print(f'Pontuações de cada fold: {cv_scores}')\n",
    "    print(f'Média da Acurácia: {np.mean(cv_scores)}')\n",
    "    print(f'Desvio Padrão da Acurácia: {np.std(cv_scores)}')\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    return rf_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1940a717-e700-4733-b8a6-aa85cb8b683c",
   "metadata": {},
   "source": [
    "## 1.9 - Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7341e8f-6f43-499d-97d6-e3cecbdcef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    precision = precision_score(y, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y, predictions, average='weighted', zero_division=0)\n",
    "    return accuracy, precision, recall, f1, predictions\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    # Gera a matriz de confusão\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    cm = cm.astype('int')  # Converte para inteiros para garantir o formato correto\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, cbar=False)\n",
    "\n",
    "    plt.title('Matriz de detecção de intrusões')\n",
    "    plt.xlabel('Previstos')\n",
    "    plt.ylabel('Reais')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7110ce-ab57-4164-8c6a-913e17b3acfc",
   "metadata": {},
   "source": [
    "## 1.10 - Geração da Curva de Aprendizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78cea6-51da-4699-9315-0aa13c3b5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, train_sizes=train_sizes, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Curva de Aprendizado\")\n",
    "    plt.xlabel(\"Tamanho do Conjunto de Treinamento\")\n",
    "    plt.ylabel(scoring.capitalize())\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Pontuação de Treinamento\")\n",
    "\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Pontuação de Validação\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365b138-10c1-4006-887b-19d4fc4b0801",
   "metadata": {},
   "source": [
    "# 2 - Realização do Algoritmo Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3577d-1713-4138-a0b7-2d3c2d53a437",
   "metadata": {},
   "source": [
    "## 2.1 - Carregamento e pré-processamento dos dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b0faf-5ee4-4e9b-a9bc-2c107fdcbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar e pré-processar os dados de treinamento\n",
    "df_train = load_data('Train_data.csv')\n",
    "df_train, encoder, imputer, scaler, numeric_cols = preprocess_training_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcf371-6d67-4108-9f77-a84f86c2a166",
   "metadata": {},
   "source": [
    "## 2.2 - Divisão de dados em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483d176-6972-45d7-83bb-00cac4386c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = split_and_balance_data(df_train, method='smote')\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40900fc2-7ac5-48db-9199-edc8b4f178c9",
   "metadata": {},
   "source": [
    "## 2.3 - Criação e Ajuste do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf0fcb-95b9-4a22-a223-3b356b2c1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o pipeline\n",
    "pipeline = create_pipeline(X_train)\n",
    "\n",
    "# Ajustar o pipeline com os dados de treinamento\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70772a-77c3-45c7-8d44-77458809b4a7",
   "metadata": {},
   "source": [
    "## 2.4 - Geração de Curvas de aprendizado utilizando validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b2ef6-060a-463c-bd34-c428aa98dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizar e treinar o modelo usando o GridSearchCV no pipeline\n",
    "optimized_model = pipeline(X_train, y_train)\n",
    "\n",
    "\n",
    "# Ajustar e treinar o modelo usando validação cruzada estratificada\n",
    "trained_model = train_model_with_cross_validation(X_train, y_train)\n",
    "\n",
    "# Gerar curva de aprendizado\n",
    "plot_learning_curve(pipeline, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1715d9-4988-423b-8605-2f4cb71b6bf4",
   "metadata": {},
   "source": [
    "## 2.5 - Avaliação do modelo no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3563b-2b1e-4a42-8f00-68658d48301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo no conjunto de teste\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_predictions = evaluate_model(pipeline, X_test, y_test)\n",
    "print(\"Métricas de avaliação no conjunto de teste:\")\n",
    "print(\"Acurácia:\", test_accuracy)\n",
    "print(\"Precisão:\", test_precision)\n",
    "print(\"Recall:\", test_recall)\n",
    "print(\"F1-score:\", test_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679f69e-65c4-4056-a664-ad63abfeb7bf",
   "metadata": {},
   "source": [
    "## 2.6 - Geração dos ataques DoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe893a-bf81-4de5-9377-ae1bfd1eb0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realização dos ataques Dos \n",
    "hping3 10.117.17.19 -S -p 80 --flood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e83212-e489-43af-b951-460d520fce2b",
   "metadata": {},
   "source": [
    "### 2.6.0 - Extração de informações dos pacotes de ataque capturados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef735a1c-1221-4462-9eba-c5b97731ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tshark -r \"novos-ataques-dos.pcap\"\n",
    "-T fields -E header=y -E separator=, -E quote=d -E occurrence=f\n",
    "-e ip.src -e ip.dst -e ip.proto -e ip.checksum -e tcp.srcport -e tcp.dstport -e tcp.flags -e frame.len -e frame.time_epoch\n",
    "> novos-ataques-dos.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544f0a6-d3c1-433b-b430-c65404d391c6",
   "metadata": {},
   "source": [
    "### 2.6.1 - Importação e Leitura dos dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830bab0-7f0c-4865-a840-e9649d9d60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import sniff, IP, TCP\n",
    "dados_extraidos = pd.read_csv('novo-ataques-dos.csv')\n",
    "dados_extraidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85445f8d-3234-4916-a6c6-4fc156009757",
   "metadata": {},
   "source": [
    "### 2.6.2 -  Definição dos Dicionários de Mapeamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe2841-9b84-423c-ba93-015746fc0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento entre números de protocolo e seus respectivos nomes\n",
    "protocol_mapping = {\n",
    "    1: \"icmp\",\n",
    "    6: \"tcp\",\n",
    "    17: \"udp\"\n",
    "}\n",
    "\n",
    "# Mapeamento de portas para serviços\n",
    "port_to_service = {\n",
    "    20: \"ftp\",\n",
    "    21: \"ftp\",\n",
    "    22: \"ssh\",\n",
    "    23: \"telnet\",\n",
    "    25: \"smtp\",\n",
    "    53: \"domain\",\n",
    "    80: \"http\",\n",
    "    110: \"pop3\",\n",
    "    143: \"imap\",\n",
    "    443: \"https\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7385c0-a616-4a33-9e2d-98a13ae9f0df",
   "metadata": {},
   "source": [
    "### 2.6.3 - Definição de Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efe836-1133-43cf-9151-39c9464d5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter o número do protocolo em texto\n",
    "def convert_protocol(proto_number):\n",
    "    return protocol_mapping.get(proto_number, \"Unknown\")\n",
    "\n",
    "# Função para determinar o serviço com base na porta\n",
    "def get_service(port):\n",
    "    return port_to_service.get(port, \"other\")\n",
    "\n",
    "# Função para inferir flags baseado nos dados disponíveis\n",
    "def infer_flag(df):\n",
    "    flags = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['protocol_type'] == \"tcp\":\n",
    "            if row['src_port'] < row['dst_port']:\n",
    "                flags.append('SF')\n",
    "            else:\n",
    "                flags.append('S0')\n",
    "        else:\n",
    "            flags.append('OTH')\n",
    "    return flags\n",
    "\n",
    "# Função para inferir se é um ataque do tipo land\n",
    "def infer_land(df):\n",
    "    lands = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['src_ip'] == row['dst_ip']:\n",
    "            lands.append(1)\n",
    "        else:\n",
    "            lands.append(0)\n",
    "    return lands\n",
    "\n",
    "# Função para inferir fragmentos errados\n",
    "def infer_wrong_fragment(df):\n",
    "    wrong_fragments = []\n",
    "    for _, row in df.iterrows():\n",
    "        if 'ip.flags.df' in df.columns and row['ip.flags.df'] == 0 and row['ip.frag_offset'] != 0:\n",
    "            wrong_fragments.append(1)\n",
    "        else:\n",
    "            wrong_fragments.append(0)\n",
    "    return wrong_fragments\n",
    "\n",
    "\n",
    "# Função para inferir a coluna 'urgent'\n",
    "def infer_urgent(df):\n",
    "    urgents = []\n",
    "    for _, row in df.iterrows():\n",
    "        if 'tcp.flags.urg' in df.columns:\n",
    "            urgents.append(row['tcp.flags.urg'])\n",
    "        else:\n",
    "            urgents.append(0)\n",
    "    return urgents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef9302-7ce3-4b6d-a0a0-faf2722fc6d6",
   "metadata": {},
   "source": [
    "### 2.6.4 - Cálculo de Métricas por Conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2b365-d923-45ce-a93e-ffc116a35132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular 'count' e 'srv_count'\n",
    "def calculate_counts(df):\n",
    "    df['count'] = df.apply(lambda row: ((df['dst_ip'] == row['dst_ip']) & \n",
    "                                        (df['timestamp'] <= row['timestamp']) & \n",
    "                                        (df['timestamp'] > row['timestamp'] - pd.Timedelta(seconds=2))).sum(), axis=1)\n",
    "    \n",
    "    df['srv_count'] = df.apply(lambda row: ((df['dst_port'] == row['dst_port']) & \n",
    "                                            (df['timestamp'] <= row['timestamp']) & \n",
    "                                            (df['timestamp'] > row['timestamp'] - pd.Timedelta(seconds=2))).sum(), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e02f96-3936-4747-8f13-01c5452ea317",
   "metadata": {},
   "source": [
    "### 2.6.5 - Cálculo de Taxas de Erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccb135-d807-4fe8-8754-15c1e6d17963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular Serror Rate e Srv Serror Rate\n",
    "def calculate_serror_rates(df):\n",
    "    df['serror_rate'] = df.apply(lambda row: ((df['dst_ip'] == row['dst_ip']) & \n",
    "                                              (df['flag'] == 'S0')).sum() / row['count'] if row['count'] > 0 else 0, axis=1)\n",
    "    \n",
    "    df['srv_serror_rate'] = df.apply(lambda row: ((df['dst_port'] == row['dst_port']) & \n",
    "                                                  (df['flag'] == 'S0')).sum() / row['srv_count'] if row['srv_count'] > 0 else 0, axis=1)\n",
    "    return df\n",
    "\n",
    "# Função para calcular Rerror Rate e Srv Rerror Rate\n",
    "def calculate_rerror_rates(df):\n",
    "    df['rerror_rate'] = df.apply(lambda row: ((df['dst_ip'] == row['dst_ip']) & \n",
    "                                              (df['flag'] == 'REJ')).sum() / row['count'] if row['count'] > 0 else 0, axis=1)\n",
    "    \n",
    "    df['srv_rerror_rate'] = df.apply(lambda row: ((df['dst_port'] == row['dst_port']) & \n",
    "                                                  (df['flag'] == 'REJ')).sum() / row['srv_count'] if row['srv_count'] > 0 else 0, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b456afd-0133-423a-830b-3a30761ca3d2",
   "metadata": {},
   "source": [
    "### 2.6.6 - Cálculo de Taxas de Serviço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1cfd19-4870-449e-9dde-4e06ca2af821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular Same Srv Rate e Diff Srv Rate\n",
    "def calculate_srv_rates(df):\n",
    "    df['same_srv_rate'] = df.apply(lambda row: ((df['dst_ip'] == row['dst_ip']) & \n",
    "                                                (df['service'] == row['service'])).sum() / row['count'] if row['count'] > 0 else 0, axis=1)\n",
    "    \n",
    "    df['diff_srv_rate'] = df.apply(lambda row: ((df['dst_ip'] == row['dst_ip']) & \n",
    "                                                (df['service'] != row['service'])).sum() / row['count'] if row['count'] > 0 else 0, axis=1)\n",
    "    return df\n",
    "\n",
    "# Função para calcular Srv Diff Host Rate\n",
    "def calculate_srv_diff_host_rate(df):\n",
    "    df['srv_diff_host_rate'] = df.apply(lambda row: ((df['dst_ip'] != row['dst_ip']) & \n",
    "                                                     (df['service'] == row['service'])).sum() / row['srv_count'] if row['srv_count'] > 0 else 0, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c552a0-a8c3-45d5-bc52-92848d1323aa",
   "metadata": {},
   "source": [
    "### 2.6.7 - Carregamento e Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee065964-85af-460b-8e56-2fb98a90780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o CSV extraído\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\igorm\\\\Downloads\\\\Implementacao-TCC\\\\novos-ataques-dos.csv\")\n",
    "\n",
    "# Converter frame.time_epoch para datetime\n",
    "df['frame.time_epoch'] = pd.to_datetime(df['frame.time_epoch'], unit='s')\n",
    "\n",
    "# Converter o campo ip.proto para texto\n",
    "df['ip.proto'] = df['ip.proto'].apply(convert_protocol)\n",
    "\n",
    "# Calcular a duração, src_bytes e dst_bytes por conexão individual\n",
    "df['duration'] = df.groupby(['ip.src', 'ip.dst', 'ip.proto'])['frame.time_epoch'].transform(lambda x: (x.max() - x.min()).total_seconds())\n",
    "df['src_bytes'] = df.groupby(['ip.src', 'ip.dst', 'ip.proto'])['frame.len'].transform('sum')\n",
    "df['dst_bytes'] = df.groupby(['ip.dst', 'ip.src', 'ip.proto'])['frame.len'].transform('sum')\n",
    "\n",
    "# Adicionar a coluna 'service' baseada na porta de destino\n",
    "df['service'] = df['tcp.dstport'].apply(get_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd6183-31ae-428e-93c1-b5bff8dab708",
   "metadata": {},
   "source": [
    "### 2.6.8 - Cálculo das Métricas do Host de Destino "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ba5ee-39e6-4973-b0f3-52d0800bbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular as métricas Dst Host\n",
    "def calculate_dst_host_metrics(df):\n",
    "    df['dst_host_count'] = df.groupby('dst_ip')['dst_ip'].transform('count')\n",
    "    df['dst_host_srv_count'] = df.groupby(['dst_ip', 'service'])['service'].transform('count')\n",
    "    df['dst_host_same_srv_rate'] = df['dst_host_srv_count'] / df['dst_host_count']\n",
    "    df['dst_host_diff_srv_rate'] = df.groupby('dst_ip')['service'].transform(lambda x: x.nunique()) / df['dst_host_count']\n",
    "    df['dst_host_same_src_port_rate'] = df.groupby('dst_ip')['src_port'].transform(lambda x: x.nunique()) / df['dst_host_count']\n",
    "    df['dst_host_srv_diff_host_rate'] = df.groupby('service')['dst_ip'].transform(lambda x: x.nunique()) / df['dst_host_count']\n",
    "    df['dst_host_serror_rate'] = df.groupby('dst_ip')['flag'].transform(lambda x: (x == 'S0').sum()) / df['dst_host_count']\n",
    "    df['dst_host_srv_serror_rate'] = df.groupby(['dst_ip', 'service'])['flag'].transform(lambda x: (x == 'S0').sum()) / df['dst_host_srv_count']\n",
    "    df['dst_host_rerror_rate'] = df.groupby('dst_ip')['flag'].transform(lambda x: (x == 'REJ').sum()) / df['dst_host_count']\n",
    "    df['dst_host_srv_rerror_rate'] = df.groupby(['dst_ip', 'service'])['flag'].transform(lambda x: (x == 'REJ').sum()) / df['dst_host_srv_count']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3235e2-a004-426b-ba1e-a08445779d23",
   "metadata": {},
   "source": [
    "### 2.6.9 - Renomeação das colunas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4697f8-bf60-4823-a1ab-6274bff319b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear colunas para corresponder aos nomes NSL-KDD\n",
    "df.rename(columns={\n",
    "    'ip.src': 'src_ip',\n",
    "    'ip.dst': 'dst_ip',\n",
    "    'ip.proto': 'protocol_type',\n",
    "    'tcp.srcport': 'src_port',\n",
    "    'tcp.dstport': 'dst_port',\n",
    "    'frame.len': 'frame_length',\n",
    "    'frame.time_epoch': 'timestamp'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf58dce-977d-4e29-a2a2-5e6cf5c2d7d1",
   "metadata": {},
   "source": [
    "### 2.6.10 - Realização dos cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6681a22-b41c-4fb6-a5c0-fe35d26b01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inferir a coluna 'flag' após renomear as colunas\n",
    "df['flag'] = infer_flag(df)\n",
    "\n",
    "# Inferir a coluna 'land'\n",
    "df['land'] = infer_land(df)\n",
    "\n",
    "# Inferir a coluna 'wrong_fragment'\n",
    "df['wrong_fragment'] = infer_wrong_fragment(df)\n",
    "\n",
    "# Inferir a coluna 'urgent'\n",
    "df['urgent'] = infer_urgent(df)\n",
    "\n",
    "## Calcular 'count' e 'srv_count'\n",
    "df = calculate_counts(df)\n",
    "\n",
    "# Calcular Serror Rate e Srv Serror Rate\n",
    "df = calculate_serror_rates(df)\n",
    "\n",
    "# Calcular Rerror Rate e Srv Rerror Rate\n",
    "df = calculate_rerror_rates(df)\n",
    "\n",
    "# Calcular Same Srv Rate e Diff Srv Rate\n",
    "df = calculate_srv_rates(df)\n",
    "\n",
    "# Calcular Srv Diff Host Rate\n",
    "df = calculate_srv_diff_host_rate(df)\n",
    "\n",
    "# Calcular métricas específicas do host de destino\n",
    "df = calculate_dst_host_metrics(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c0993-d662-4144-bdd8-218159e8801b",
   "metadata": {},
   "source": [
    "### 2.6.11 - Salvamento e Exibição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9afa55-25eb-4d74-b7b2-ec8a8982ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas as colunas necessárias\n",
    "df = df[['src_ip', 'dst_ip', 'protocol_type', 'src_port', 'dst_port', 'duration', 'src_bytes', 'dst_bytes', 'service', 'flag']]\n",
    "\n",
    "# Salvar o CSV processado\n",
    "df.to_csv(\"novos-ataques-dos.csv\", index=False)\n",
    "\n",
    "# Apresentar os dados na tela\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e50ed-1f82-4b51-86f7-732643e7d431",
   "metadata": {},
   "source": [
    "### 2.6.12 - Definição dos IPs considerados como anomalias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad9c0b-e22e-4e57-ba98-b4fecc9e8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import ipaddress\n",
    "\n",
    "# Função para gerar um IP aleatório\n",
    "def random_ip():\n",
    "    return str(ipaddress.IPv4Address(random.randint(0, (1 << 32) - 1)))\n",
    "\n",
    "# Caminho do arquivo CSV original\n",
    "csv_file = 'novos-ataques-dos.csv'\n",
    "\n",
    "# Carregar o arquivo CSV em um DataFrame do pandas\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# IPs alvo a serem classificados como anomalias\n",
    "target_ip_1 = \"10.117.77.119\"\n",
    "target_ip_2 = \"10.117.77.120\"\n",
    "\n",
    "# Garantir que todos os valores da coluna 'ip.src' sejam strings\n",
    "df['ip.src'] = df['ip.src'].astype(str)\n",
    "\n",
    "# Adicionar coluna 'class' e modificar IPs de origem\n",
    "df['class'] = df['ip.src'].apply(lambda x: 'anomaly' if x == target_ip_1 or x == target_ip_2 else 'normal')\n",
    "df['ip.src'] = df.apply(lambda row: random_ip() if row['ip.src'] == target_ip_1 else row['ip.src'], axis=1)\n",
    "\n",
    "# Caminho do novo arquivo CSV\n",
    "output_csv_file = 'dados-coletados.csv'\n",
    "\n",
    "# Salvar o DataFrame modificado em um novo arquivo CSV\n",
    "df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(\"Arquivo CSV modificado gerado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08f39e-b59f-470f-80e1-a7b8b54b85e4",
   "metadata": {},
   "source": [
    "## 2.7 - Carregamento e pré-processamento do novo conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f5101-264c-4496-a904-81f0fea7abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o novo conjunto de dados\n",
    "df_new = load_data(\"dados-coletados.csv\")\n",
    "\n",
    "# Fazer o pré-processamento nos dados novos\n",
    "df_new = preprocess_new_data(df_new, encoder=encoder, imputer=imputer, scaler=scaler, numeric_cols=numeric_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339dfc0b-1cdd-4a36-af29-b439e7518673",
   "metadata": {},
   "source": [
    "## 2.8 - Adição e alinhamento das colunas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9d88b-d41b-4a84-abd5-b55bc6d691d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verificação de presença da coluna 'class' após o pré-processamento\n",
    "if 'class' not in df_new.columns:\n",
    "    print(\"A coluna 'class' não foi encontrada após o pré-processamento.\")\n",
    "\n",
    "\n",
    "if 'class' in df_new.columns:\n",
    "    y_new = df_new['class']\n",
    "    X_new = df_new.drop(['class'], axis=1)\n",
    "    \n",
    "    X_new = align_and_add_missing_columns(X_new, reference_columns=X_train_val.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f11b51-34b7-4395-a41a-d1d489f1d3e6",
   "metadata": {},
   "source": [
    "## 2.9 - Realização e Avaliação de previsões dos novos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c0db5-1e23-47f9-82df-1b7caf12309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fazer previsões nos dados novos\n",
    "    new_predictions = pipeline.predict(df_new)\n",
    "    \n",
    "    df_new['predictions'] = new_predictions\n",
    "    \n",
    "    accuracy, precision, recall, f1, new_predictions = evaluate_model(pipeline, X_new, y_new)\n",
    "    print(\"Métricas de avaliação no novo conjunto de dados:\")\n",
    "    print(\"Acurácia:\", accuracy)\n",
    "    print(\"Precisão:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "    plot_confusion_matrix(y_new, new_predictions, labels=np.unique(y_new))\n",
    "else:\n",
    "    df_new = align_and_add_missing_columns(df_new, reference_columns=X_train_val.columns)\n",
    "    \n",
    "  \n",
    "    print(\"Predições no novo conjunto de dados (sem rótulos verdadeiros):\")\n",
    "    print(df_new['predictions'].value_counts())\n",
    "\n",
    "# Exibir mais linhas do DataFrame com previsões e métricas\n",
    "print(df_new.head())\n",
    "\n",
    "# Salvar o DataFrame com previsões e métricas em um novo arquivo CSV\n",
    "try:\n",
    "    df_new.to_csv(\"dados-finais-previstos.csv\", index=False)\n",
    "    print(f\"As previsões e métricas foram geradas e salvas.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao salvar o arquivo: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
